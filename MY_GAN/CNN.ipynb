{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input,Dense,Embedding,Conv2D,MaxPool2D\n",
    "from keras.layers import Reshape,Flatten,Dropout,Concatenate\n",
    "from keras.callbacks import ModelCheckpoint,TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from data_helpers import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "print('Loading data')\n",
    "x, y, vocabulary, vocabulary_inv = load_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split( x, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8529, 56)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.shape(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sequence_length=x.shape[1]\n",
    "vocabulary_size = len(vocabulary_inv) # 18765\n",
    "embedding_dim = 16\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 512\n",
    "drop = 0.5\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_3/embedding_lookup/Identity:0\", shape=(?, 56, 16), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"reshape_1/Reshape:0\", shape=(?, 56, 16, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "inputs=Input(shape=(sequence_length,),dtype='int32')\n",
    "embedding=Embedding(input_dim=vocabulary_size,output_dim=embedding_dim,\n",
    "                    input_length=sequence_length)(inputs)\n",
    "print(embedding)\n",
    "reshape=Reshape((sequence_length,embedding_dim,1))(embedding)\n",
    "print(reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning Model...\n"
     ]
    }
   ],
   "source": [
    "conv_0=Conv2D(num_filters,kernel_size=(filter_sizes[0],embedding_dim),\n",
    "              padding='valid' ,kernel_initializer='normal',activation='relu' \n",
    "              )(reshape)\n",
    "pool_0=MaxPool2D(pool_size=(sequence_length-filter_sizes[0]+1,1),\n",
    "                 strides=(1,1),padding='valid')(conv_0)\n",
    "conv_1=Conv2D(num_filters,kernel_size=(filter_sizes[1],embedding_dim),\n",
    "              padding='valid' ,kernel_initializer='normal',activation='relu' \n",
    "              )(reshape)\n",
    "pool_1=MaxPool2D(pool_size=(sequence_length-filter_sizes[1]+1,1),\n",
    "                 strides=(1,1),padding='valid')(conv_1)\n",
    "conv_2=Conv2D(num_filters,kernel_size=(filter_sizes[2],embedding_dim),\n",
    "              padding='valid' ,kernel_initializer='normal',activation='relu' \n",
    "              )(reshape)\n",
    "pool_2=MaxPool2D(pool_size=(sequence_length-filter_sizes[2]+1,1),\n",
    "                 strides=(1,1),padding='valid')(conv_2)\n",
    "#-----------进行全连接\n",
    "concatenated_tensor=Concatenate(axis=1)([pool_0,pool_1,pool_2])\n",
    "flatten=Flatten()(concatenated_tensor)\n",
    "dropout=Dropout(drop)(flatten)\n",
    "output=Dense(units=2,activation='softmax')(dropout)\n",
    "model =Model(inputs=inputs,outputs=output)\n",
    "\n",
    "log_dir = \"./logs/\"\n",
    "#记录所有训练过程，每隔一定步数记录最大值\n",
    "tensorboard = TensorBoard(log_dir=log_dir,histogram_freq=0)\n",
    "checkpoint = ModelCheckpoint(log_dir + \"best_weights.h5\",\n",
    "                                 monitor=\"val_loss\",\n",
    "                                 mode='min',\n",
    "                                 save_weights_only=True,\n",
    "                                 save_best_only=True,\n",
    "                                 verbose=1,\n",
    "                                 period=1)\n",
    "\n",
    "callback_lists=[tensorboard,checkpoint]\n",
    "\n",
    "#定义优化器\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(adam,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "import os\n",
    "target_dir='./models/'\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "model.save('./models/model.h5')\n",
    "model.save_weights('./models/weights.h5')\n",
    "print(\"Traning Model...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8529 samples, validate on 2133 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 9s - loss: 0.6928 - acc: 0.5100 - val_loss: 0.6923 - val_acc: 0.5021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch 00001: val_loss improved from inf to 0.69228, saving model to ./logs/best_weights.h5\nEpoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 8s - loss: 0.6891 - acc: 0.5613 - val_loss: 0.6879 - val_acc: 0.6203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch 00002: val_loss improved from 0.69228 to 0.68791, saving model to ./logs/best_weights.h5\nEpoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 8s - loss: 0.6786 - acc: 0.6413 - val_loss: 0.6744 - val_acc: 0.6493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch 00003: val_loss improved from 0.68791 to 0.67438, saving model to ./logs/best_weights.h5\nEpoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 8s - loss: 0.6436 - acc: 0.7103 - val_loss: 0.6329 - val_acc: 0.6864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch 00004: val_loss improved from 0.67438 to 0.63288, saving model to ./logs/best_weights.h5\nEpoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 8s - loss: 0.5604 - acc: 0.7686 - val_loss: 0.5696 - val_acc: 0.7159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nEpoch 00005: val_loss improved from 0.63288 to 0.56956, saving model to ./logs/best_weights.h5\n{'val_loss': [0.6922848684878289, 0.6879065004079151, 0.6743843218445275, 0.6328807091746484, 0.5695642030859464], 'val_acc': [0.5021097108449278, 0.6202531708024558, 0.6493202104179501, 0.686357243570765, 0.7158931080466752], 'loss': [0.6927885451536551, 0.689063959041197, 0.6786201501286788, 0.6436335173996661, 0.5604007385102452], 'acc': [0.5100246293000508, 0.5612615856053416, 0.6413413136725773, 0.7102825662492658, 0.768554343541575]}\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train,y_train,batch_size=batch_size,epochs=epochs\n",
    "                  ,verbose=2,callbacks=callback_lists,validation_data=(X_test,y_test))\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6922848684878289, 0.6879065004079151, 0.6743843218445275, 0.6328807091746484, 0.5695642030859464]\n"
     ]
    }
   ],
   "source": [
    "print(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5332981  0.4667019 ]\n [0.3247114  0.6752886 ]\n [0.5276256  0.47237432]\n ...\n [0.22835843 0.77164155]\n [0.4378191  0.5621809 ]\n [0.4697877  0.53021234]]\n"
     ]
    }
   ],
   "source": [
    "#-----------------predict------------------------\n",
    "from keras.models import Sequential,load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from data_helpers import load_data\n",
    "x, y, vocabulary, vocabulary_inv = load_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split( x, y, test_size=0.2, random_state=42)\n",
    "model_path='./models/model.h5'\n",
    "model_weigths_path='./logs/best_weights.h5'\n",
    "load_model(model_path)\n",
    "model.load_weights(model_weigths_path)\n",
    "y=model.predict(X_test)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
